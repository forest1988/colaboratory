{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_my_modifications_for_HuggingFace_transformers.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNj6rOyXecVv3OVDsmm9RFb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/forest1988/colaboratory/blob/main/test_my_modifications_for_HuggingFace_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP5IQyCszpIF"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N6h3sDGz7LK",
        "outputId": "41958acc-ed98-4737-c035-540fb6ff9dd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "if os.path.exists(\"transformers\"):\n",
        "  print(\"The repository is already cloned. Remove and re-clone it.\")\n",
        "  shutil.rmtree(\"transformers\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The repository is already cloned. Remove and re-clone it.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AE5COGDH0FAh",
        "outputId": "e14d5370-ca12-4a27-8cf3-4336f6d14ab7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone -b forest1988-prophetnet-prepare-seq2seq-batch https://github.com/forest1988/transformers.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Counting objects: 100% (43/43), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 50008 (delta 20), reused 28 (delta 13), pack-reused 49965\u001b[K\n",
            "Receiving objects: 100% (50008/50008), 36.85 MiB | 19.28 MiB/s, done.\n",
            "Resolving deltas: 100% (34903/34903), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsd0giWu0LZb",
        "outputId": "000e03b5-6ca8-4956-dda7-473d94de91e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/transformers/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nCJs-oM0j2n",
        "outputId": "1a8fbed7-b87f-41dd-b9f2-6a4d87f75290",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -e \".[dev]\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/transformers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (20.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (0.7)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (3.12.4)\n",
            "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (0.9.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (1.18.5)\n",
            "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (0.1.91)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (4.41.1)\n",
            "Requirement already satisfied: keras2onnx; extra == \"dev\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (1.7.0)\n",
            "Requirement already satisfied: sphinx-rtd-theme==0.4.3; extra == \"dev\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (0.4.3)\n",
            "Requirement already satisfied: pytest-xdist; extra == \"dev\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (2.1.0)\n",
            "Requirement already satisfied: sphinx==3.2.1; extra == \"dev\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (3.2.1)\n",
            "Requirement already satisfied: parameterized; extra == \"dev\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (0.7.4)\n",
            "Requirement already satisfied: flake8>=3.8.3; extra == \"dev\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (3.8.4)\n",
            "Requirement already satisfied: sphinx-copybutton; extra == \"dev\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (0.3.1)\n",
            "Requirement already satisfied: tensorflow>=2.0; extra == \"dev\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (2.3.0)\n",
            "Requirement already satisfied: jaxlib==0.1.55; extra == \"dev\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (0.1.55)\n",
            "Requirement already satisfied: timeout-decorator; extra == \"dev\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (0.4.1)\n",
            "Requirement already satisfied: faiss-cpu; extra == \"dev\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (1.6.4.post2)\n",
            "Requirement already satisfied: unidic-lite>=1.0.7; extra == \"dev\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (1.0.7)\n",
            "Requirement already satisfied: onnxconverter-common; extra == \"dev\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (1.7.0)\n",
            "Requirement already satisfied: ipadic<2.0,>=1.0.0; extra == \"dev\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (1.0.0)\n",
            "Requirement already satisfied: datasets; extra == \"dev\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (1.1.2)\n",
            "Requirement already satisfied: recommonmark; extra == \"dev\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (0.6.0)\n",
            "Requirement already satisfied: psutil; extra == \"dev\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (5.4.8)\n",
            "Requirement already satisfied: pytest; extra == \"dev\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (6.1.2)\n",
            "Requirement already satisfied: black>=20.8b1; extra == \"dev\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (20.8b1)\n",
            "Requirement already satisfied: unidic>=1.0.2; extra == \"dev\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (1.0.3)\n",
            "Requirement already satisfied: torch>=1.0; extra == \"dev\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: scikit-learn; extra == \"dev\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (0.22.2.post1)\n",
            "Requirement already satisfied: jax>=0.2.0; extra == \"dev\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (0.2.4)\n",
            "Requirement already satisfied: flax==0.2.2; extra == \"dev\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (0.2.2)\n",
            "Requirement already satisfied: fugashi>=1.0; extra == \"dev\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (1.0.5)\n",
            "Requirement already satisfied: sphinx-markdown-tables; extra == \"dev\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (0.0.15)\n",
            "Requirement already satisfied: isort>=5.5.4; extra == \"dev\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.0) (5.6.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.5.0) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.5.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.0) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.0) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers==3.5.0) (50.3.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.0) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.0) (3.0.4)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.6/dist-packages (from keras2onnx; extra == \"dev\"->transformers==3.5.0) (1.8.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.6/dist-packages (from keras2onnx; extra == \"dev\"->transformers==3.5.0) (0.3.1)\n",
            "Requirement already satisfied: execnet>=1.1 in /usr/local/lib/python3.6/dist-packages (from pytest-xdist; extra == \"dev\"->transformers==3.5.0) (1.7.1)\n",
            "Requirement already satisfied: pytest-forked in /usr/local/lib/python3.6/dist-packages (from pytest-xdist; extra == \"dev\"->transformers==3.5.0) (1.3.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx==3.2.1; extra == \"dev\"->transformers==3.5.0) (2.8.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx==3.2.1; extra == \"dev\"->transformers==3.5.0) (1.2.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx==3.2.1; extra == \"dev\"->transformers==3.5.0) (2.6.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx==3.2.1; extra == \"dev\"->transformers==3.5.0) (0.7.12)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.6/dist-packages (from sphinx==3.2.1; extra == \"dev\"->transformers==3.5.0) (1.0.2)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.6/dist-packages (from sphinx==3.2.1; extra == \"dev\"->transformers==3.5.0) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.6/dist-packages (from sphinx==3.2.1; extra == \"dev\"->transformers==3.5.0) (1.0.3)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx==3.2.1; extra == \"dev\"->transformers==3.5.0) (2.0.0)\n",
            "Requirement already satisfied: docutils>=0.12 in /usr/local/lib/python3.6/dist-packages (from sphinx==3.2.1; extra == \"dev\"->transformers==3.5.0) (0.16)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.6/dist-packages (from sphinx==3.2.1; extra == \"dev\"->transformers==3.5.0) (1.0.2)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp in /usr/local/lib/python3.6/dist-packages (from sphinx==3.2.1; extra == \"dev\"->transformers==3.5.0) (1.0.3)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.6/dist-packages (from sphinx==3.2.1; extra == \"dev\"->transformers==3.5.0) (1.1.4)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from sphinx==3.2.1; extra == \"dev\"->transformers==3.5.0) (2.11.2)\n",
            "Requirement already satisfied: pycodestyle<2.7.0,>=2.6.0a1 in /usr/local/lib/python3.6/dist-packages (from flake8>=3.8.3; extra == \"dev\"->transformers==3.5.0) (2.6.0)\n",
            "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from flake8>=3.8.3; extra == \"dev\"->transformers==3.5.0) (0.6.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from flake8>=3.8.3; extra == \"dev\"->transformers==3.5.0) (2.0.0)\n",
            "Requirement already satisfied: pyflakes<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from flake8>=3.8.3; extra == \"dev\"->transformers==3.5.0) (2.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0; extra == \"dev\"->transformers==3.5.0) (0.10.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0; extra == \"dev\"->transformers==3.5.0) (1.6.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0; extra == \"dev\"->transformers==3.5.0) (1.33.2)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0; extra == \"dev\"->transformers==3.5.0) (1.4.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0; extra == \"dev\"->transformers==3.5.0) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0; extra == \"dev\"->transformers==3.5.0) (1.1.2)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0; extra == \"dev\"->transformers==3.5.0) (2.10.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0; extra == \"dev\"->transformers==3.5.0) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0; extra == \"dev\"->transformers==3.5.0) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0; extra == \"dev\"->transformers==3.5.0) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0; extra == \"dev\"->transformers==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0; extra == \"dev\"->transformers==3.5.0) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0; extra == \"dev\"->transformers==3.5.0) (0.35.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.0; extra == \"dev\"->transformers==3.5.0) (0.3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets; extra == \"dev\"->transformers==3.5.0) (1.1.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets; extra == \"dev\"->transformers==3.5.0) (0.70.10)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets; extra == \"dev\"->transformers==3.5.0) (2.0.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets; extra == \"dev\"->transformers==3.5.0) (2.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets; extra == \"dev\"->transformers==3.5.0) (0.3.3)\n",
            "Requirement already satisfied: commonmark>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from recommonmark; extra == \"dev\"->transformers==3.5.0) (0.9.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest; extra == \"dev\"->transformers==3.5.0) (20.2.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.6/dist-packages (from pytest; extra == \"dev\"->transformers==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from pytest; extra == \"dev\"->transformers==3.5.0) (1.9.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.6/dist-packages (from pytest; extra == \"dev\"->transformers==3.5.0) (0.10.2)\n",
            "Requirement already satisfied: pluggy<1.0,>=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest; extra == \"dev\"->transformers==3.5.0) (0.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.6/dist-packages (from black>=20.8b1; extra == \"dev\"->transformers==3.5.0) (3.7.4.3)\n",
            "Requirement already satisfied: typed-ast>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from black>=20.8b1; extra == \"dev\"->transformers==3.5.0) (1.4.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.6/dist-packages (from black>=20.8b1; extra == \"dev\"->transformers==3.5.0) (0.4.3)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.6/dist-packages (from black>=20.8b1; extra == \"dev\"->transformers==3.5.0) (1.4.4)\n",
            "Requirement already satisfied: pathspec<1,>=0.6 in /usr/local/lib/python3.6/dist-packages (from black>=20.8b1; extra == \"dev\"->transformers==3.5.0) (0.8.1)\n",
            "Requirement already satisfied: wasabi<1.0.0,>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from unidic>=1.0.2; extra == \"dev\"->transformers==3.5.0) (0.8.0)\n",
            "Requirement already satisfied: plac<2.0.0,>=1.1.3 in /usr/local/lib/python3.6/dist-packages (from unidic>=1.0.2; extra == \"dev\"->transformers==3.5.0) (1.1.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0; extra == \"dev\"->transformers==3.5.0) (0.16.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from flax==0.2.2; extra == \"dev\"->transformers==3.5.0) (3.2.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.6/dist-packages (from flax==0.2.2; extra == \"dev\"->transformers==3.5.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from sphinx-markdown-tables; extra == \"dev\"->transformers==3.5.0) (3.3.3)\n",
            "Requirement already satisfied: apipkg>=1.4 in /usr/local/lib/python3.6/dist-packages (from execnet>=1.1->pytest-xdist; extra == \"dev\"->transformers==3.5.0) (1.5)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.6/dist-packages (from babel>=1.3->sphinx==3.2.1; extra == \"dev\"->transformers==3.5.0) (2018.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->sphinx==3.2.1; extra == \"dev\"->transformers==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->flake8>=3.8.3; extra == \"dev\"->transformers==3.5.0) (3.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.0; extra == \"dev\"->transformers==3.5.0) (0.4.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.0; extra == \"dev\"->transformers==3.5.0) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.0; extra == \"dev\"->transformers==3.5.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.0; extra == \"dev\"->transformers==3.5.0) (1.17.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets; extra == \"dev\"->transformers==3.5.0) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->flax==0.2.2; extra == \"dev\"->transformers==3.5.0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->flax==0.2.2; extra == \"dev\"->transformers==3.5.0) (0.10.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.0; extra == \"dev\"->transformers==3.5.0) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.0; extra == \"dev\"->transformers==3.5.0) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.0; extra == \"dev\"->transformers==3.5.0) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.0; extra == \"dev\"->transformers==3.5.0) (0.2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.0; extra == \"dev\"->transformers==3.5.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.0; extra == \"dev\"->transformers==3.5.0) (0.4.8)\n",
            "Installing collected packages: transformers\n",
            "  Found existing installation: transformers 3.5.0\n",
            "    Can't uninstall 'transformers'. No files were found to uninstall.\n",
            "  Running setup.py develop for transformers\n",
            "Successfully installed transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GzboMuO0CJq",
        "outputId": "54983b5f-8685-4e81-fe9a-a099ef637f7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# to avoid the \"ModuleNotFoundError\" \n",
        "import sys\n",
        "print(sys.path)\n",
        "sys.path.append('/content/transformers/src')\n",
        "print(sys.path)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', '/env/python', '/usr/lib/python36.zip', '/usr/lib/python3.6', '/usr/lib/python3.6/lib-dynload', '/usr/local/lib/python3.6/dist-packages', '/content/transformers/src', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.6/dist-packages/IPython/extensions', '/root/.ipython']\n",
            "['', '/env/python', '/usr/lib/python36.zip', '/usr/lib/python3.6', '/usr/lib/python3.6/lib-dynload', '/usr/local/lib/python3.6/dist-packages', '/content/transformers/src', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.6/dist-packages/IPython/extensions', '/root/.ipython', '/content/transformers/src']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5CoJZeE1Ycq",
        "outputId": "450e90ea-245a-4459-e863-4753f7f37897",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install --upgrade pytest"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pytest in /usr/local/lib/python3.6/dist-packages (6.1.2)\n",
            "Requirement already satisfied, skipping upgrade: py>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from pytest) (1.9.0)\n",
            "Requirement already satisfied, skipping upgrade: iniconfig in /usr/local/lib/python3.6/dist-packages (from pytest) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: toml in /usr/local/lib/python3.6/dist-packages (from pytest) (0.10.2)\n",
            "Requirement already satisfied, skipping upgrade: pluggy<1.0,>=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest) (0.13.1)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest) (20.2.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from pytest) (20.4)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest) (3.4.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->pytest) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from packaging->pytest) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DCwFdkt0PW_",
        "outputId": "d8af6512-7452-4a7f-c054-d0014bc6e1bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python -m pytest tests/test_tokenization_prophetnet.py"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.6.9, pytest-6.1.2, py-1.9.0, pluggy-0.13.1\n",
            "rootdir: /content/transformers\n",
            "plugins: xdist-2.1.0, forked-1.3.0, typeguard-2.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 72 items                                                             \u001b[0m\n",
            "\n",
            "tests/test_tokenization_prophetnet.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33ms\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m [ 47%]\n",
            "\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33ms\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33ms\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33ms\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33ms\u001b[0m\u001b[32m.\u001b[0m\u001b[33m                                   [100%]\u001b[0m\n",
            "\n",
            "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
            "tests/test_tokenization_prophetnet.py::ProphetNetTokenizationTest::test_padding_to_max_length\n",
            "  /content/transformers/src/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "    FutureWarning,\n",
            "\n",
            "tests/test_tokenization_prophetnet.py::ProphetNetTokenizationTest::test_prepare_seq2seq_batch\n",
            "  /content/transformers/src/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
            "    FutureWarning,\n",
            "\n",
            "-- Docs: https://docs.pytest.org/en/stable/warnings.html\n",
            "\u001b[33m================== \u001b[32m67 passed\u001b[0m, \u001b[33m\u001b[1m5 skipped\u001b[0m, \u001b[33m\u001b[1m2 warnings\u001b[0m\u001b[33m in 1.65s\u001b[0m\u001b[33m ===================\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-z9C4K72cSk",
        "outputId": "99b704de-5301-4c38-924b-a2dfab621b03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!make quality"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "black --check examples tests src utils\n",
            "\u001b[1mwould reformat /content/transformers/tests/test_tokenization_prophetnet.py\u001b[0m\n",
            "\u001b[1mOh no! üí• üíî üí•\u001b[0m\n",
            "\u001b[1m1 file would be reformatted\u001b[0m, 564 files would be left unchanged.\u001b[0m\n",
            "Makefile:27: recipe for target 'quality' failed\n",
            "make: *** [quality] Error 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ozvl8FN3hjp",
        "outputId": "10493fb0-4445-4ed1-875d-609ea7dbccae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!make style"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "black examples tests src utils\n",
            "\u001b[1mreformatted /content/transformers/tests/test_tokenization_prophetnet.py\u001b[0m\n",
            "\u001b[1mAll done! ‚ú® üç∞ ‚ú®\u001b[0m\n",
            "\u001b[1m1 file reformatted\u001b[0m, 564 files left unchanged.\u001b[0m\n",
            "isort examples tests src utils\n",
            "python utils/style_doc.py src/transformers docs/source --max_len 119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYDbJxtC34bI",
        "outputId": "87e323ec-2ac5-4255-9305-48dcb54efa76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!cat /content/transformers/tests/test_tokenization_prophetnet.py"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# coding=utf-8\n",
            "# Copyright 2020 The HuggingFace Inc. team, The Microsoft Research team.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "\n",
            "\n",
            "import os\n",
            "import unittest\n",
            "\n",
            "from transformers import BatchEncoding\n",
            "from transformers.testing_utils import require_torch, slow\n",
            "from transformers.tokenization_bert import (\n",
            "    BasicTokenizer,\n",
            "    WordpieceTokenizer,\n",
            "    _is_control,\n",
            "    _is_punctuation,\n",
            "    _is_whitespace,\n",
            ")\n",
            "from transformers.tokenization_prophetnet import VOCAB_FILES_NAMES, ProphetNetTokenizer\n",
            "\n",
            "from .test_tokenization_common import TokenizerTesterMixin\n",
            "\n",
            "\n",
            "class ProphetNetTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n",
            "\n",
            "    tokenizer_class = ProphetNetTokenizer\n",
            "    test_rust_tokenizer = False\n",
            "\n",
            "    def setUp(self):\n",
            "        super().setUp()\n",
            "\n",
            "        vocab_tokens = [\n",
            "            \"[UNK]\",\n",
            "            \"[CLS]\",\n",
            "            \"[SEP]\",\n",
            "            \"[PAD]\",\n",
            "            \"[MASK]\",\n",
            "            \"want\",\n",
            "            \"##want\",\n",
            "            \"##ed\",\n",
            "            \"wa\",\n",
            "            \"un\",\n",
            "            \"runn\",\n",
            "            \"##ing\",\n",
            "            \",\",\n",
            "            \"low\",\n",
            "            \"lowest\",\n",
            "        ]\n",
            "        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
            "        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n",
            "            vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n",
            "\n",
            "    def get_input_output_texts(self, tokenizer):\n",
            "        input_text = \"UNwant\\u00E9d,running\"\n",
            "        output_text = \"unwanted, running\"\n",
            "        return input_text, output_text\n",
            "\n",
            "    def test_full_tokenizer(self):\n",
            "        tokenizer = self.tokenizer_class(self.vocab_file)\n",
            "\n",
            "        tokens = tokenizer.tokenize(\"UNwant\\u00E9d,running\")\n",
            "        self.assertListEqual(tokens, [\"un\", \"##want\", \"##ed\", \",\", \"runn\", \"##ing\"])\n",
            "        self.assertListEqual(tokenizer.convert_tokens_to_ids(tokens), [9, 6, 7, 12, 10, 11])\n",
            "\n",
            "    def test_chinese(self):\n",
            "        tokenizer = BasicTokenizer()\n",
            "\n",
            "        self.assertListEqual(tokenizer.tokenize(\"ah\\u535A\\u63A8zz\"), [\"ah\", \"\\u535A\", \"\\u63A8\", \"zz\"])\n",
            "\n",
            "    def test_basic_tokenizer_lower(self):\n",
            "        tokenizer = BasicTokenizer(do_lower_case=True)\n",
            "\n",
            "        self.assertListEqual(\n",
            "            tokenizer.tokenize(\" \\tHeLLo!how  \\n Are yoU?  \"), [\"hello\", \"!\", \"how\", \"are\", \"you\", \"?\"]\n",
            "        )\n",
            "        self.assertListEqual(tokenizer.tokenize(\"H\\u00E9llo\"), [\"hello\"])\n",
            "\n",
            "    def test_basic_tokenizer_lower_strip_accents_false(self):\n",
            "        tokenizer = BasicTokenizer(do_lower_case=True, strip_accents=False)\n",
            "\n",
            "        self.assertListEqual(\n",
            "            tokenizer.tokenize(\" \\tH√§LLo!how  \\n Are yoU?  \"), [\"h√§llo\", \"!\", \"how\", \"are\", \"you\", \"?\"]\n",
            "        )\n",
            "        self.assertListEqual(tokenizer.tokenize(\"H\\u00E9llo\"), [\"h\\u00E9llo\"])\n",
            "\n",
            "    def test_basic_tokenizer_lower_strip_accents_true(self):\n",
            "        tokenizer = BasicTokenizer(do_lower_case=True, strip_accents=True)\n",
            "\n",
            "        self.assertListEqual(\n",
            "            tokenizer.tokenize(\" \\tH√§LLo!how  \\n Are yoU?  \"), [\"hallo\", \"!\", \"how\", \"are\", \"you\", \"?\"]\n",
            "        )\n",
            "        self.assertListEqual(tokenizer.tokenize(\"H\\u00E9llo\"), [\"hello\"])\n",
            "\n",
            "    def test_basic_tokenizer_lower_strip_accents_default(self):\n",
            "        tokenizer = BasicTokenizer(do_lower_case=True)\n",
            "\n",
            "        self.assertListEqual(\n",
            "            tokenizer.tokenize(\" \\tH√§LLo!how  \\n Are yoU?  \"), [\"hallo\", \"!\", \"how\", \"are\", \"you\", \"?\"]\n",
            "        )\n",
            "        self.assertListEqual(tokenizer.tokenize(\"H\\u00E9llo\"), [\"hello\"])\n",
            "\n",
            "    def test_basic_tokenizer_no_lower(self):\n",
            "        tokenizer = BasicTokenizer(do_lower_case=False)\n",
            "\n",
            "        self.assertListEqual(\n",
            "            tokenizer.tokenize(\" \\tHeLLo!how  \\n Are yoU?  \"), [\"HeLLo\", \"!\", \"how\", \"Are\", \"yoU\", \"?\"]\n",
            "        )\n",
            "\n",
            "    def test_basic_tokenizer_no_lower_strip_accents_false(self):\n",
            "        tokenizer = BasicTokenizer(do_lower_case=False, strip_accents=False)\n",
            "\n",
            "        self.assertListEqual(\n",
            "            tokenizer.tokenize(\" \\tH√§LLo!how  \\n Are yoU?  \"), [\"H√§LLo\", \"!\", \"how\", \"Are\", \"yoU\", \"?\"]\n",
            "        )\n",
            "\n",
            "    def test_basic_tokenizer_no_lower_strip_accents_true(self):\n",
            "        tokenizer = BasicTokenizer(do_lower_case=False, strip_accents=True)\n",
            "\n",
            "        self.assertListEqual(\n",
            "            tokenizer.tokenize(\" \\tH√§LLo!how  \\n Are yoU?  \"), [\"HaLLo\", \"!\", \"how\", \"Are\", \"yoU\", \"?\"]\n",
            "        )\n",
            "\n",
            "    def test_basic_tokenizer_respects_never_split_tokens(self):\n",
            "        tokenizer = BasicTokenizer(do_lower_case=False, never_split=[\"[UNK]\"])\n",
            "\n",
            "        self.assertListEqual(\n",
            "            tokenizer.tokenize(\" \\tHeLLo!how  \\n Are yoU? [UNK]\"), [\"HeLLo\", \"!\", \"how\", \"Are\", \"yoU\", \"?\", \"[UNK]\"]\n",
            "        )\n",
            "\n",
            "    def test_wordpiece_tokenizer(self):\n",
            "        vocab_tokens = [\"[UNK]\", \"[CLS]\", \"[SEP]\", \"want\", \"##want\", \"##ed\", \"wa\", \"un\", \"runn\", \"##ing\"]\n",
            "\n",
            "        vocab = {}\n",
            "        for (i, token) in enumerate(vocab_tokens):\n",
            "            vocab[token] = i\n",
            "        tokenizer = WordpieceTokenizer(vocab=vocab, unk_token=\"[UNK]\")\n",
            "\n",
            "        self.assertListEqual(tokenizer.tokenize(\"\"), [])\n",
            "\n",
            "        self.assertListEqual(tokenizer.tokenize(\"unwanted running\"), [\"un\", \"##want\", \"##ed\", \"runn\", \"##ing\"])\n",
            "\n",
            "        self.assertListEqual(tokenizer.tokenize(\"unwantedX running\"), [\"[UNK]\", \"runn\", \"##ing\"])\n",
            "\n",
            "    @require_torch\n",
            "    def test_prepare_seq2seq_batch(self):\n",
            "        tokenizer = self.tokenizer_class.from_pretrained(\"microsoft/prophetnet-large-uncased\")\n",
            "\n",
            "        src_text = [\"A long paragraph for summarization.\", \"Another paragraph for summarization.\"]\n",
            "        tgt_text = [\n",
            "            \"Summary of the text.\",\n",
            "            \"Another summary.\",\n",
            "        ]\n",
            "        expected_src_tokens = [1037, 2146, 20423, 2005, 7680, 7849, 3989, 1012, 102]\n",
            "        batch = tokenizer.prepare_seq2seq_batch(\n",
            "            src_text,\n",
            "            tgt_texts=tgt_text,\n",
            "            return_tensors=\"pt\",\n",
            "        )\n",
            "        self.assertIsInstance(batch, BatchEncoding)\n",
            "        result = list(batch.input_ids.numpy()[0])\n",
            "        self.assertListEqual(expected_src_tokens, result)\n",
            "\n",
            "        self.assertEqual((2, 9), batch.input_ids.shape)\n",
            "        self.assertEqual((2, 9), batch.attention_mask.shape)\n",
            "\n",
            "    def test_is_whitespace(self):\n",
            "        self.assertTrue(_is_whitespace(\" \"))\n",
            "        self.assertTrue(_is_whitespace(\"\\t\"))\n",
            "        self.assertTrue(_is_whitespace(\"\\r\"))\n",
            "        self.assertTrue(_is_whitespace(\"\\n\"))\n",
            "        self.assertTrue(_is_whitespace(\"\\u00A0\"))\n",
            "\n",
            "        self.assertFalse(_is_whitespace(\"A\"))\n",
            "        self.assertFalse(_is_whitespace(\"-\"))\n",
            "\n",
            "    def test_is_control(self):\n",
            "        self.assertTrue(_is_control(\"\\u0005\"))\n",
            "\n",
            "        self.assertFalse(_is_control(\"A\"))\n",
            "        self.assertFalse(_is_control(\" \"))\n",
            "        self.assertFalse(_is_control(\"\\t\"))\n",
            "        self.assertFalse(_is_control(\"\\r\"))\n",
            "\n",
            "    def test_is_punctuation(self):\n",
            "        self.assertTrue(_is_punctuation(\"-\"))\n",
            "        self.assertTrue(_is_punctuation(\"$\"))\n",
            "        self.assertTrue(_is_punctuation(\"`\"))\n",
            "        self.assertTrue(_is_punctuation(\".\"))\n",
            "\n",
            "        self.assertFalse(_is_punctuation(\"A\"))\n",
            "        self.assertFalse(_is_punctuation(\" \"))\n",
            "\n",
            "    @slow\n",
            "    def test_sequence_builders(self):\n",
            "        tokenizer = self.tokenizer_class.from_pretrained(\"microsoft/prophetnet-large-uncased\")\n",
            "\n",
            "        text = tokenizer.encode(\"sequence builders\", add_special_tokens=False)\n",
            "        text_2 = tokenizer.encode(\"multi-sequence build\", add_special_tokens=False)\n",
            "\n",
            "        encoded_sentence = tokenizer.build_inputs_with_special_tokens(text)\n",
            "        encoded_pair = tokenizer.build_inputs_with_special_tokens(text, text_2)\n",
            "\n",
            "        assert encoded_sentence == text + [102]\n",
            "        assert encoded_pair == text + [102] + text_2 + [102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg6Url52389g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}